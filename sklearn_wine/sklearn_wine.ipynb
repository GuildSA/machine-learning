{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://elitedatascience.com/python-machine-learning-tutorial-scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " >>> data.shape <<< \n",
      "\n",
      "(1599, 12)\n",
      "\n",
      " >>> data.head <<< \n",
      "\n",
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n",
      "\n",
      " >>> data.describe <<< \n",
      "\n",
      "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
      "mean        8.319637          0.527821     0.270976        2.538806   \n",
      "std         1.741096          0.179060     0.194801        1.409928   \n",
      "min         4.600000          0.120000     0.000000        0.900000   \n",
      "25%         7.100000          0.390000     0.090000        1.900000   \n",
      "50%         7.900000          0.520000     0.260000        2.200000   \n",
      "75%         9.200000          0.640000     0.420000        2.600000   \n",
      "max        15.900000          1.580000     1.000000       15.500000   \n",
      "\n",
      "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
      "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
      "mean      0.087467            15.874922             46.467792     0.996747   \n",
      "std       0.047065            10.460157             32.895324     0.001887   \n",
      "min       0.012000             1.000000              6.000000     0.990070   \n",
      "25%       0.070000             7.000000             22.000000     0.995600   \n",
      "50%       0.079000            14.000000             38.000000     0.996750   \n",
      "75%       0.090000            21.000000             62.000000     0.997835   \n",
      "max       0.611000            72.000000            289.000000     1.003690   \n",
      "\n",
      "                pH    sulphates      alcohol      quality  \n",
      "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
      "mean      3.311113     0.658149    10.422983     5.636023  \n",
      "std       0.154386     0.169507     1.065668     0.807569  \n",
      "min       2.740000     0.330000     8.400000     3.000000  \n",
      "25%       3.210000     0.550000     9.500000     5.000000  \n",
      "50%       3.310000     0.620000    10.200000     6.000000  \n",
      "75%       3.400000     0.730000    11.100000     6.000000  \n",
      "max       4.010000     2.000000    14.900000     8.000000  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Load the data set:\n",
    "\n",
    "data = pd.read_csv(\"winequality_red.csv\", sep=';')\n",
    "\n",
    "print(\"\\n >>> data.shape <<< \\n\")\n",
    "print(data.shape)\n",
    "\n",
    "print(\"\\n >>> data.head <<< \\n\")\n",
    "print(data.head())\n",
    "\n",
    "print(\"\\n >>> data.describe <<< \\n\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " >>> X_train_scaled.mean <<< \n",
      "\n",
      "[ 1.16664562e-16 -3.05550043e-17 -8.47206937e-17 -2.22218213e-17\n",
      "  2.22218213e-17 -6.38877362e-17 -4.16659149e-18 -2.54439854e-15\n",
      " -8.70817622e-16 -4.08325966e-16 -1.17220107e-15]\n",
      "\n",
      " >>> X_train_scaled.std <<< \n",
      "\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      " >>> X_test_scaled.mean <<< \n",
      "\n",
      "[ 0.02776704  0.02592492 -0.03078587 -0.03137977 -0.00471876 -0.04413827\n",
      " -0.02414174 -0.00293273 -0.00467444 -0.10894663  0.01043391]\n",
      "\n",
      " >>> X_test_scaled.std <<< \n",
      "\n",
      "[1.02160495 1.00135689 0.97456598 0.91099054 0.86716698 0.94193125\n",
      " 1.03673213 1.03145119 0.95734849 0.83829505 1.0286218 ]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# We'll need to split our data into training and test sets. Splitting the data \n",
    "# at the beginning of your modeling workflow is crucial for getting a realistic \n",
    "# estimate of your model's performance.\n",
    "\n",
    "# First, let's separate our target (y) features from our input (X) features:\n",
    "# This will allow us to take advantage of Scikit-Learn's useful train_test_split \n",
    "# function in the next step:\n",
    "\n",
    "y = data.quality\n",
    "X = data.drop('quality', axis=1)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Next, split data into training and test sets:\n",
    "\n",
    "# As you can see, we'll set aside 20% of the data as a test set for evaluating \n",
    "# our model. We also set an arbitrary \"random state\" (a.k.a. seed) so that we \n",
    "# can reproduce our results. Finally, it's good practice to stratify your sample \n",
    "# by the target variable. This will ensure your training set looks similar to \n",
    "# your test set, making your evaluation metrics more reliable.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=123, \n",
    "                                                    stratify=y)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Data Preprocessing:\n",
    "\n",
    "# Next, we need to standardize or scale our features. Standardization is the \n",
    "# process of subtracting the means from each feature and then dividing by the \n",
    "# feature standard deviations. This is very common procedure many ML algorithms\n",
    "# assume that all features are centered around zero and have approximately the \n",
    "# same variance.\n",
    "\n",
    "# We'll use Scikit-Learn's Transformer API to scale the data. After scaling, the \n",
    "# 'scaler' object below will have the saved means and standard deviations for each \n",
    "# feature in the training set.\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "# Let's confirm that it worked by applying the transformer to the training data.\n",
    "\n",
    "# Applying transformer to training data.\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "print(\"\\n >>> X_train_scaled.mean <<< \\n\")\n",
    "print(X_train_scaled.mean(axis=0))\n",
    "\n",
    "print(\"\\n >>> X_train_scaled.std <<< \\n\")\n",
    "print(X_train_scaled.std(axis=0))\n",
    "\n",
    "# Applying transformer to test data.\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n >>> X_test_scaled.mean <<< \\n\")\n",
    "print(X_test_scaled.mean(axis=0))\n",
    "\n",
    "print(\"\\n >>> X_test_scaled.std <<< \\n\")\n",
    "print(X_test_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " >>> pipeline.get_params <<< \n",
      "\n",
      "{'memory': None, 'steps': [('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "                      max_features='auto', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                      n_jobs=None, oob_score=False, random_state=None,\n",
      "                      verbose=0, warm_start=False))], 'verbose': False, 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True), 'randomforestregressor': RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "                      max_features='auto', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                      n_jobs=None, oob_score=False, random_state=None,\n",
      "                      verbose=0, warm_start=False), 'standardscaler__copy': True, 'standardscaler__with_mean': True, 'standardscaler__with_std': True, 'randomforestregressor__bootstrap': True, 'randomforestregressor__criterion': 'mse', 'randomforestregressor__max_depth': None, 'randomforestregressor__max_features': 'auto', 'randomforestregressor__max_leaf_nodes': None, 'randomforestregressor__min_impurity_decrease': 0.0, 'randomforestregressor__min_impurity_split': None, 'randomforestregressor__min_samples_leaf': 1, 'randomforestregressor__min_samples_split': 2, 'randomforestregressor__min_weight_fraction_leaf': 0.0, 'randomforestregressor__n_estimators': 100, 'randomforestregressor__n_jobs': None, 'randomforestregressor__oob_score': False, 'randomforestregressor__random_state': None, 'randomforestregressor__verbose': 0, 'randomforestregressor__warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Create pipeline and declare hyperparameters to tune:\n",
    "\n",
    "# This is exactly what it looks like: a modeling pipeline that first transforms \n",
    "# the data using StandardScaler() and then fits a model using a random forest regressor.\n",
    "\n",
    "pipeline = make_pipeline(preprocessing.StandardScaler(), RandomForestRegressor(n_estimators=100))\n",
    "\n",
    "# List tunable hyperparameters.\n",
    "print(\"\\n >>> pipeline.get_params <<< \\n\")\n",
    "print(pipeline.get_params())\n",
    "\n",
    "# Now, let's declare the hyperparameters we want to tune through cross-validation.\n",
    "\n",
    "hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                    'randomforestregressor__max_depth': [None, 5, 3, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " >>> clf.best_params_ <<< \n",
      "\n",
      "{'randomforestregressor__max_depth': None, 'randomforestregressor__max_features': 'auto'}\n",
      "\n",
      " >>> clf.refit <<< \n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Tune model using a cross-validation pipeline.\n",
    "\n",
    "# GridSearchCV to setup cross-validation on our pipeline. GridSearchCV essentially \n",
    "# performs cross-validation across the entire \"grid\" (all possible permutations)\n",
    "# of hyperparameters.\n",
    "\n",
    "clf = GridSearchCV(pipeline, hyperparameters, cv=10)\n",
    " \n",
    "# Fit and tune model.\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Now, we can see the best set of parameters found using CV:\n",
    "print(\"\\n >>> clf.best_params_ <<< \\n\")\n",
    "print(clf.best_params_)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Refit on the entire training set.\n",
    "\n",
    "# After you've tuned your hyperparameters appropriately using cross-validation, \n",
    "# you can generally get a small performance improvement by refitting the model \n",
    "# on the entire training set.\n",
    "\n",
    "# Conveniently, GridSearchCV from sklearn will automatically refit the model \n",
    "# with the best set of hyperparameters using the entire training set.\n",
    "\n",
    "# This functionality is ON by default, but you can confirm it:\n",
    "\n",
    "# Confirm model will be retrained.\n",
    "print(\"\\n >>> clf.refit <<< \\n\")\n",
    "print(clf.refit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " >>> r2_score(y_test, y_pred) <<< \n",
      "\n",
      "0.46062546158968476\n",
      "\n",
      " >>> mean_squared_error(y_test, y_pred) <<< \n",
      "\n",
      "0.3480440625\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Evaluate model pipeline on test data.\n",
    "\n",
    "# Now, we can predict a new set of data:\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# After we predict, we can use the metrics we imported earlier to evaluate our \n",
    "# model performance.\n",
    "\n",
    "print(\"\\n >>> r2_score(y_test, y_pred) <<< \\n\")\n",
    "print(r2_score(y_test, y_pred))\n",
    "\n",
    "print(\"\\n >>> mean_squared_error(y_test, y_pred) <<< \\n\")\n",
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_regressor.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Save model to .pkl file. for future use.\n",
    "\n",
    "joblib.dump(clf, 'rf_regressor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " >>> r2_score(y_test, y_pred) <<< \n",
      "\n",
      "0.46062546158968476\n",
      "\n",
      " >>> mean_squared_error(y_test, y_pred) <<< \n",
      "\n",
      "0.3480440625\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# To use the model in the future - load it from the .pkl file.\n",
    "\n",
    "clf2 = joblib.load('rf_regressor.pkl')\n",
    "\n",
    "# Predict data set using loaded model.\n",
    "clf2.predict(X_test)\n",
    "\n",
    "# After we predict, we can use the metrics we imported earlier to evaluate our \n",
    "# model performance.\n",
    "\n",
    "print(\"\\n >>> r2_score(y_test, y_pred) <<< \\n\")\n",
    "print(r2_score(y_test, y_pred))\n",
    "\n",
    "print(\"\\n >>> mean_squared_error(y_test, y_pred) <<< \\n\")\n",
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
